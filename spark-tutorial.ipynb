{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1756e41-5086-4f6d-ae5a-547d127ae78c",
   "metadata": {},
   "source": [
    "# Using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353617ac-951d-4ade-a363-3f2009253060",
   "metadata": {},
   "source": [
    "Apache Spark is one of the most well-known multilanguage Big Data processing frameworks. With the creation of the Resilient Distributed Dataset (RDD)\n",
    "and the introduction of in-memory computing, it has transformed the way in which big data is processed today. While the base API is in Scala, Apache Spark also provides APIs in Java, Python and R.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b> Learning Objectives </b>\n",
    "    <ul>\n",
    "        <li>Learn about PySpark's two main datastructure APIs (i.e. RDD and DataFrames)</li>\n",
    "        <li> Understand how to rewrite tasks into map-reduce form</li>\n",
    "        <li> Clean data and train a model using PySpark's ML API</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f728b-4eb9-4ecb-9369-4a42b0352b29",
   "metadata": {},
   "source": [
    "## How to install PySpark\n",
    "\n",
    "PySpark has been already installed on all the VMs, but can be installed using `pip install pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef930d3-e960-4857-b2ef-2576e2470b6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to configure PySpark to run in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4f4430-59f5-4d61-a867-4b6de292f6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: IPYTHON=1\n",
      "env: PYSPARK_PYTHON=/home/valeriehayot/ericsson/dask/.venv/bin/python3\n",
      "env: PYSPARK_DRIVER_PYTHON=/home/valeriehayot/ericsson/dask/.venv/bin/ipython\n",
      "env: PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.13.0.8-1.fc34.x86_64\n"
     ]
    }
   ],
   "source": [
    "# some magic to make sure Spark is configured properly\n",
    "import subprocess as sp\n",
    "\n",
    "python_home = sp.check_output([\"which\", \"python3\"], encoding=\"UTF-8\")\n",
    "ipython_home = sp.check_output([\"which\", \"ipython\"], encoding=\"UTF-8\")\n",
    "java_home = sp.check_output(\"dirname $(dirname $(readlink -f $(which java)))\", shell=True, encoding=\"UTF-8\")\n",
    "\n",
    "%env IPYTHON=1\n",
    "%env PYSPARK_PYTHON={python_home}\n",
    "%env PYSPARK_DRIVER_PYTHON={ipython_home}\n",
    "%env PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "%env JAVA_HOME={java_home}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d4f4c-3d22-4112-a294-cb1f9bc44698",
   "metadata": {},
   "source": [
    "## PySpark Datastructure APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51273b48-c979-4817-8f93-957413a6fa80",
   "metadata": {},
   "source": [
    "PySpark has two main datastructure APIs to facilitate data processing. These are:\n",
    "- Resilient Distributed Dataset (RDD)\n",
    "- DataFrames\n",
    "\n",
    "In this section, we will discuss what they are and how to use them efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151de7e-3803-4511-8426-a17b2cf5124c",
   "metadata": {},
   "source": [
    "### Resilient Distributed Datasets (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a599151-83ea-45d1-8191-1590c3712f56",
   "metadata": {},
   "source": [
    "RDDs are the base datastructure in Spark. They can be viewed as a distributed collection of lists.\n",
    "RDDs can be used similarly to the Dask Bags we learned about last week.<br>\n",
    "\n",
    "RDDs are:\n",
    "- immutable\n",
    "- lazily evaluated\n",
    "- maintain intermediate results in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d8dbe-310b-405d-af22-d47f310cf262",
   "metadata": {},
   "source": [
    "#### How to create a Spark RDD context\n",
    "\n",
    "A SparkContext provides information to Spark on how to access the cluster. It uses information provided in the `SparkConf` to inform it of the application requirements.\n",
    "\n",
    "Below we are informing Spark to create a local context (i.e. use the Spark Standalone Scheduler) with just one core. To specify all the cores,\n",
    "we would specify `local[*]` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ac7f43-4685-4111-95d3-b67f0147f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/24 20:08:17 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 172.31.172.55 instead (on interface wlp2s0)\n",
      "21/11/24 20:08:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/valeriehayot/ericsson/dask/.venv/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/24 20:08:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"My Spark Application\").setMaster(\"local[1]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb297fe0-011c-4c08-b63a-7608b035fccf",
   "metadata": {},
   "source": [
    "Now that our Spark Context has been created, we can now access the dashboard through a browser on `<VM IP>:4040` to view our application's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1549a720-653a-48dd-9850-b0e2dae0fb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark dashboard url: http://127.0.0.1:4040\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "url = f\"http://{socket.gethostbyname(hostname)}:4040\"\n",
    "print(\"Spark dashboard url:\", url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a5736-e81b-48d1-9c09-151616609b42",
   "metadata": {},
   "source": [
    "Using the above context, we can now create an RDD. Below we will create an RDD from a Python iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6042903b-d8a9-4cd1-8fc5-c6af572eb1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bdc53-6540-46ea-bd5b-9728c1d60aba",
   "metadata": {},
   "source": [
    "As we saw last week in Dask, Spark RDDs are also lazily evaluated. To compute the result, we must apply the `collect()` action to our RDD object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7587f51-c03c-4791-a1f2-eddc838e9a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc44b50-b084-4a44-83e1-d52608fd139b",
   "metadata": {},
   "source": [
    "To split up our list into chunks to be processed separately in Spark, we can provide a second parameter in `parallelize` (numSlices) to break up the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03372e11-ffdb-46df-a77a-245ef3e11a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10), numSlices=5)\n",
    "rdd.collect() # can now check the dashboard to see that chunks were processed individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdec70-30d8-431c-93e0-da34695ff5de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b> Some Spark terminology: </b>\n",
    "    <ul>\n",
    "        <li>Functions which create RDDs from existing ones are known as <b>transformations</b>. Examples of transformations include <code>map</code>, <code>filter</code>, <code>reduceByKey</code></li>\n",
    "        <li>Functions which compute the results and return them to the driver are known as <b>actions</b>. Examples of actions include <code>reduce</code> and <code>collect</code>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af90d6-8b2d-431f-97d6-77f3d9962201",
   "metadata": {},
   "source": [
    "#### The MapReduce Programming model\n",
    "\n",
    "Many parallel problems can be expressed in map-reduce form. Map and reduce are not unique to Apache Spark or Big Data frameworks. In fact, they were first introduced by functional programming languages.\n",
    "\n",
    "Map operations apply a common function across all the elements within a collection\n",
    "(e.g. all the elements of an RDD or all the RDD partitions, in the case of Spark). Reduce operations perform an aggregration (e.g. sum, max, count) across all the elements.\n",
    "\n",
    "A traditional example of a MapReduce use case is Word Count. The aim of Word Count is to obtain the count of all the unique words in a document.\n",
    "\n",
    "A sequential implementation of word count could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637804ff-68e9-4021-b3e4-6ee1a43b797f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5),\n",
       " ('of', 5),\n",
       " ('the', 5),\n",
       " ('count', 4),\n",
       " ('is', 3),\n",
       " ('word', 3),\n",
       " ('all', 2),\n",
       " ('in', 2),\n",
       " ('mapreduce', 2),\n",
       " ('to', 2),\n",
       " ('aggretation', 1),\n",
       " ('aim', 1),\n",
       " ('an', 1),\n",
       " ('and', 1),\n",
       " ('apply', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Many parallel problems can be expressed in map-reduce form. \n",
    "          That is, apply a general function to all the elements (map)\n",
    "          and then perform an aggretation of the previous results.\n",
    "          A traditional example of a MapReduce use case is Word Count.\n",
    "          The aim of Word Count is to obtain the count of all the unique\n",
    "          words in a document.\n",
    "          A sequential implementation of word count could look something like this\"\"\"\n",
    "\n",
    "\n",
    "def word_count(counter, word):\n",
    "    if word in counter:\n",
    "        counter[word] += 1\n",
    "    else:\n",
    "        counter[word] = 1\n",
    "\n",
    "# clean all the special characters from the text assuming we don't want them in our word count\n",
    "# can be ommitted if we want special characters to remain\n",
    "clean_text = \"\".join([char for char in text if char.isalnum() or char == \" \"])\n",
    "counter = {}\n",
    "for word in clean_text.split():\n",
    "       word_count(counter, word.lower()) # convert word to lowercase. Can be ommitted\n",
    "        \n",
    "sorted(counter.items(), key=lambda x: [-x[1], x[0]])[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1628777-6ba0-45a2-9a55-bf9030510527",
   "metadata": {},
   "source": [
    "As we can see in the above example, we first need to apply a the count of 1 to each word, and then we need to sum the counts of each word. \n",
    "\n",
    "Within Python, functional programming constructs such as map/reduce also exist. Let's rewrite the above sequential code using Python's `map` and `reduce` functions. (note: map is a Python built-in, whereas reduce can be found within the `functools` library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f14ad14-a4eb-4b5c-8a95-1b364025bb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5),\n",
       " ('of', 5),\n",
       " ('the', 5),\n",
       " ('count', 4),\n",
       " ('is', 3),\n",
       " ('word', 3),\n",
       " ('all', 2),\n",
       " ('in', 2),\n",
       " ('mapreduce', 2),\n",
       " ('to', 2),\n",
       " ('aggretation', 1),\n",
       " ('aim', 1),\n",
       " ('an', 1),\n",
       " ('and', 1),\n",
       " ('apply', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from functools import reduce\n",
    "\n",
    "text = \"\"\"Many parallel problems can be expressed in map-reduce form. \n",
    "          That is, apply a general function to all the elements (map)\n",
    "          and then perform an aggretation of the previous results.\n",
    "          A traditional example of a MapReduce use case is Word Count.\n",
    "          The aim of Word Count is to obtain the count of all the unique\n",
    "          words in a document.\n",
    "          A sequential implementation of word count could look something like this\"\"\"\n",
    "\n",
    "def remove_special(word: str) -> Tuple[str, int]:\n",
    "    \"\"\"Removes all non-alphanumeric characters from words and returns its lowercase form\n",
    "    \n",
    "    Keyword arguments:\n",
    "        word - the word to process\n",
    "        \n",
    "    Returns:\n",
    "        A tuple where the first element is the lowercase word stripped of non-alphanumeric\n",
    "        characters and the second is the words initial count (always 1).\n",
    "    \"\"\"\n",
    "    word_chars= []\n",
    "    for char in word:\n",
    "        if char.isalnum():\n",
    "            word_chars.append(char)\n",
    "    \n",
    "    return (\"\".join(word_chars).lower(), 1)\n",
    "\n",
    "def reduce_by_key(accumulator: dict, entry: Tuple[str, int]) -> dict:\n",
    "    \"\"\" Adds entry to the dictionary or increments entry value if entry is already in the dictionary\n",
    "    \n",
    "    Keyword arguments:\n",
    "        accumulator - A dictionary containing the counts of all the previously seen entries\n",
    "        entry - A tuple containing the entry to add\n",
    "        \n",
    "    Returns:\n",
    "        A updated dictionary containing a count for the entry word.\n",
    "    \n",
    "    \"\"\"\n",
    "    if entry[0] in accumulator:\n",
    "        accumulator[entry[0]] += entry[1]\n",
    "    else:\n",
    "        accumulator[entry[0]] = entry[1]\n",
    "        \n",
    "    return accumulator\n",
    "    \n",
    "clean_text = map(remove_special, text.split()) # remove special characters\n",
    "clean_text = list(filter(lambda x: x != \"\", clean_text)) # filter out any empty strings as map mapping is 1-to-1\n",
    "counter = reduce(reduce_by_key, clean_text, {}) # perform a reduce along the keys\n",
    "sorted(counter.items(), key=lambda x: [-x[1], x[0]])[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be15a0-11ed-45c9-ac6f-dcbe242a82f2",
   "metadata": {},
   "source": [
    "The above functional programming can be directly mapped to Spark RDDs. In fact, it is even simpler to do using Spark as when elements in the RDD are in tuple format, they are naturally treated as key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae8c29a-34c4-4088-bc68-ca90984abb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5),\n",
       " ('of', 5),\n",
       " ('the', 5),\n",
       " ('count', 4),\n",
       " ('is', 3),\n",
       " ('word', 3),\n",
       " ('all', 2),\n",
       " ('in', 2),\n",
       " ('mapreduce', 2),\n",
       " ('to', 2),\n",
       " ('aggretation', 1),\n",
       " ('aim', 1),\n",
       " ('an', 1),\n",
       " ('and', 1),\n",
       " ('apply', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Many parallel problems can be expressed in map-reduce form. \n",
    "          That is, apply a general function to all the elements (map)\n",
    "          and then perform an aggretation of the previous results.\n",
    "          A traditional example of a MapReduce use case is Word Count.\n",
    "          The aim of Word Count is to obtain the count of all the unique\n",
    "          words in a document.\n",
    "          A sequential implementation of word count could look something like this\"\"\"\n",
    "\n",
    "# using the Spark Context created earlier\n",
    "rdd = sc.parallelize(text.split())\n",
    "cleaned_rdd = rdd.map(remove_special) # creating a new RDD using the function we created earlier\n",
    "cleaned_rdd = cleaned_rdd.filter(lambda x: x != \"\")\n",
    "counter_rdd = cleaned_rdd.reduceByKey(lambda x,y: x + y) # Spark provides a transformation to reduce over a key.\n",
    "sorted_rdd = counter_rdd.sortBy(lambda x: (-x[1], x[0])) # Note: Sorting in Spark is not always reliable as data can be shuffled\n",
    "sorted_rdd.take(15) # take is a Spark action and therefore compute the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d074b-e620-4726-afdd-f76686a0c558",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Using the provided text, return the sorted counts of all the words beginning with a unique letter.\n",
    "\n",
    "`e.g \"Hello World. hi there. foo bar.\" -> [('h', 2), ('b', 1), ('f', 1), ('t', 1), ('w', 1)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a05eb1c2-47dd-493f-985f-456648432064",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Many parallel problems can be expressed in map-reduce form. \n",
    "          That is, apply a general function to all the elements (map)\n",
    "          and then perform an aggretation of the previous results.\n",
    "          A traditional example of a MapReduce use case is Word Count.\n",
    "          The aim of Word Count is to obtain the count of all the unique\n",
    "          words in a document.\n",
    "          A sequential implementation of word count could look something like this\"\"\"\n",
    "\n",
    "\n",
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604a6ea-21ca-4cc1-bacb-2ed86d581142",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7412187-c948-4e1e-aa6b-bcf8fbbf5f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 12),\n",
       " ('t', 11),\n",
       " ('c', 7),\n",
       " ('i', 6),\n",
       " ('o', 6),\n",
       " ('m', 4),\n",
       " ('p', 4),\n",
       " ('w', 4),\n",
       " ('e', 3),\n",
       " ('f', 2),\n",
       " ('l', 2),\n",
       " ('s', 2),\n",
       " ('u', 2),\n",
       " ('b', 1),\n",
       " ('d', 1),\n",
       " ('g', 1),\n",
       " ('r', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Many parallel problems can be expressed in map-reduce form. \n",
    "          That is, apply a general function to all the elements (map)\n",
    "          and then perform an aggretation of the previous results.\n",
    "          A traditional example of a MapReduce use case is Word Count.\n",
    "          The aim of Word Count is to obtain the count of all the unique\n",
    "          words in a document.\n",
    "          A sequential implementation of word count could look something like this\"\"\"\n",
    "\n",
    "rdd = sc.parallelize(text.split())\n",
    "result = (\n",
    "            rdd.map(remove_special)\n",
    "               .filter(lambda x: x != \"\")\n",
    "               .map(lambda x: (x[0][0], x[1]))\n",
    "               .reduceByKey(lambda x,y: x+y)\n",
    "               .sortBy(lambda x: (-x[1], x[0]))\n",
    "               .collect()\n",
    "         )\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a293d3-4920-436e-b6e8-a87fe5a4df4c",
   "metadata": {},
   "source": [
    "#### Shuffling in Spark\n",
    "\n",
    "Any function that requires data to be shuffled around in Spark incurs additional overheads. All actions, which must return the results\n",
    "to the driver incur shuffling overheads, in addition to certain transformations.\n",
    "\n",
    "Transformations can be subdivided into two categories: those with narrow and those with wide dependencies. Transformations with narrow dependencies do not produce any shuffling. Examples of such transformations include `map`, `filter`, `flatMap` .\n",
    "\n",
    "Wide dependency transformations may require some amount of shuffling key-value pairs may need to be communicated with other partitions. \n",
    "All the transformations ending with `ByKey` may involve some form of shuffling (e.g. `reduceByKey`, `groupByKey`, `foldByKey`). In general, it is best to minimize both the amount of shuffling and the amount of data involved in shuffling (e.g. `reduceByKey` can be more efficient than a `groupByKey`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e943e7-417c-45fc-9f4e-5578e932937c",
   "metadata": {},
   "source": [
    "#### Overhead of Python in PySpark\n",
    "\n",
    "Just as in Python where Python built-ins are faster than user-defined functions (UDFs), Spark built-ins are significantly faster than\n",
    "their equivalent using Python UDFs as a result of the increased communication overheads between Python UDFs and Scala backend. As a result, it is best to use PySpark built-ins whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8026b6-816b-4182-b8db-f9b72eb1874e",
   "metadata": {},
   "source": [
    "<b>Example: ReduceByKey vs CountByKey on word count</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dbf49-6799-442e-b8f6-63b13b0834f8",
   "metadata": {},
   "source": [
    "ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f887a442-c4ee-4b78-a601-6d9807c03351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 ms ± 23.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "text = \"\"\"Many parallel problems can be expressed in map-reduce form. \n",
    "          That is, apply a general function to all the elements (map)\n",
    "          and then perform an aggretation of the previous results.\n",
    "          A traditional example of a MapReduce use case is Word Count.\n",
    "          The aim of Word Count is to obtain the count of all the unique\n",
    "          words in a document.\n",
    "          A sequential implementation of word count could look something like this\"\"\"\n",
    "\n",
    "(\n",
    "    sc.parallelize(text.split(), numSlices=3)\n",
    "    .map(remove_special)\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    .collectAsMap() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e433872-c628-4327-b8d7-483cc51c2f69",
   "metadata": {},
   "source": [
    "CountByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62d50506-6647-43d2-b2ee-ac2ff9e2c924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.2 ms ± 7.09 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "text = \"\"\"Many parallel problems can be expressed in map-reduce form. \n",
    "          That is, apply a general function to all the elements (map)\n",
    "          and then perform an aggretation of the previous results.\n",
    "          A traditional example of a MapReduce use case is Word Count.\n",
    "          The aim of Word Count is to obtain the count of all the unique\n",
    "          words in a document.\n",
    "          A sequential implementation of word count could look something like this\"\"\"\n",
    "\n",
    "(\n",
    "    sc.parallelize(text.split(), numSlices=3)\n",
    "    .map(remove_special)\n",
    "    .countByKey()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae945475-53b4-40ef-8f44-8eb2d38c86a8",
   "metadata": {},
   "source": [
    "#### How to terminate a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63895033-d03e-4ef3-862c-578c59cafac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503eafe-dd69-4e9a-aa0c-fb27f0800ebf",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "The Spark DataFrame is used to process structured data. It is similar to the RDD, except that the additional information on datastructure enables it to perform further optimizations.\n",
    "\n",
    "Since its parent structure is the RDD, it inherits all of the RDDs feature (i.e. immutable, lazy-evaluation, in-memory processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a105f-d8ba-4016-80c6-fb6fa938bd1b",
   "metadata": {},
   "source": [
    "#### How to start a PySpark DataFrame session\n",
    "\n",
    "Similar to the SparkContext, the SparkSession provides Spark with information on application requirements. The main difference between a SparkContext and a SparkSession is that SparkContexts create RDDs and SparkSessions create DataFrames.\n",
    "\n",
    "Whereas with a SparkContext we had to create a separate SparkConf object, with SparkSession we can set application\n",
    "properties from within the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76dde810-75b0-43b4-a4b8-21e7c5d6bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"My Spark DataFrame Program\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dd843-8580-4047-884c-ae5f3d7f7677",
   "metadata": {},
   "source": [
    "Using the created `SparkSession` we can now read a csv file. We will be reading the Iris dataset for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaad2d68-6221-4fcf-b2b0-cc86433c3a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/iris.csv\", header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da6d4f-090a-41ad-bf12-e2cf67753423",
   "metadata": {},
   "source": [
    "As with RDDs, to get the content of our DataFrame, we must use the `collect` action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53e27488-78d0-43ec-b8c1-27e99c55131f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length='5.1', sepal_width='3.5', petal_length='1.4', petal_width='0.2', species='setosa'),\n",
       " Row(sepal_length='4.9', sepal_width='3.0', petal_length='1.4', petal_width='0.2', species='setosa'),\n",
       " Row(sepal_length='4.7', sepal_width='3.2', petal_length='1.3', petal_width='0.2', species='setosa'),\n",
       " Row(sepal_length='4.6', sepal_width='3.1', petal_length='1.5', petal_width='0.2', species='setosa'),\n",
       " Row(sepal_length='5.0', sepal_width='3.6', petal_length='1.4', petal_width='0.2', species='setosa'),\n",
       " Row(sepal_length='5.4', sepal_width='3.9', petal_length='1.7', petal_width='0.4', species='setosa'),\n",
       " Row(sepal_length='4.6', sepal_width='3.4', petal_length='1.4', petal_width='0.3', species='setosa'),\n",
       " Row(sepal_length='5.0', sepal_width='3.4', petal_length='1.5', petal_width='0.2', species='setosa'),\n",
       " Row(sepal_length='4.4', sepal_width='2.9', petal_length='1.4', petal_width='0.2', species='setosa'),\n",
       " Row(sepal_length='4.9', sepal_width='3.1', petal_length='1.5', petal_width='0.1', species='setosa')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = df.collect()\n",
    "output[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a440fb-7eb1-4e44-af68-685e3a64ebc5",
   "metadata": {},
   "source": [
    "PySpark returns a list of Rows (similar to NamedTuples) rather than a DataFrame object. To access row values within the list, we can do so using their named index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d267055e-d0c3-4a63-96df-d4097faac026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].sepal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b383dee1-b69f-490e-90e7-27201f3dc92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][\"sepal_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "920b4d80-e539-4ed8-a2ea-48cf11d7a69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43532d3-df5f-4364-b0a4-08e22787693e",
   "metadata": {},
   "source": [
    "Should we want formatted output, we can use the `show` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbee7b99-0303-40e5-917a-a6d6a4e749d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b4522-5649-4db7-8d66-cd6b1980610a",
   "metadata": {},
   "source": [
    "#### How to process a PySpark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5cca5-7516-4c22-9529-46846421e5ae",
   "metadata": {},
   "source": [
    "Spark DataFrames can be manipulated in one of three ways:\n",
    "1. Using the default DataFrame API\n",
    "2. Via SQL queries\n",
    "3. Using pandas-on-spark\n",
    "\n",
    "We will perform one-hot encoding of the species column using all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9df52f55-2e9c-4b8a-8a27-22ebc4f13d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(species='virginica'), Row(species='versicolor'), Row(species='setosa')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get species list\n",
    "all_species = df.dropDuplicates([\"species\"]).select(\"species\").collect()\n",
    "all_species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7df47f-dea5-4fe4-9bf2-57b77fd353bf",
   "metadata": {},
   "source": [
    "**1. Using the default DataFrame API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d38fdec4-641b-4874-9f18-c55ca20972ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+---------+----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|virginica|versicolor|setosa|\n",
      "+------------+-----------+------------+-----------+-------+---------+----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|        0|         0|     1|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|        0|         0|     1|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|        0|         0|     1|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|        0|         0|     1|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|        0|         0|     1|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|        0|         0|     1|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|        0|         0|     1|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|        0|         0|     1|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|        0|         0|     1|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|        0|         0|     1|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|        0|         0|     1|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|        0|         0|     1|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|        0|         0|     1|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|        0|         0|     1|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|        0|         0|     1|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|        0|         0|     1|\n",
      "+------------+-----------+------------+-----------+-------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_api = None\n",
    "for species in all_species:\n",
    "    if df_api is None:\n",
    "        df_api = df.withColumn(species.species, F.when(df.species == species.species, 1).otherwise(0))\n",
    "    else:\n",
    "        df_api = df_api.withColumn(species.species, F.when(df.species == species.species, 1).otherwise(0))\n",
    "\n",
    "df_api.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91351b-0102-4967-a635-ec07fc6cfb12",
   "metadata": {},
   "source": [
    "**2. Using SQL queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca821d0f-f0e8-48ea-b8a6-74470be8f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted query:\n",
      "\n",
      " SELECT *,\n",
      "CAST(CASE WHEN species LIKE 'virginica' THEN 1 ELSE 0 END AS int) as virginica,\n",
      "CAST(CASE WHEN species LIKE 'versicolor' THEN 1 ELSE 0 END AS int) as versicolor,\n",
      "CAST(CASE WHEN species LIKE 'setosa' THEN 1 ELSE 0 END AS int) as setosa FROM table_iris \n",
      "\n",
      "+------------+-----------+------------+-----------+-------+---------+----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|virginica|versicolor|setosa|\n",
      "+------------+-----------+------------+-----------+-------+---------+----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|        0|         0|     1|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|        0|         0|     1|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|        0|         0|     1|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|        0|         0|     1|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|        0|         0|     1|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|        0|         0|     1|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|        0|         0|     1|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|        0|         0|     1|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|        0|         0|     1|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|        0|         0|     1|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|        0|         0|     1|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|        0|         0|     1|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|        0|         0|     1|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|        0|         0|     1|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|        0|         0|     1|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|        0|         0|     1|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|        0|         0|     1|\n",
      "+------------+-----------+------------+-----------+-------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"table_iris\")\n",
    "\n",
    "query = \"SELECT *\"\n",
    "\n",
    "for species in all_species:\n",
    "    query += f\", CAST(CASE WHEN species LIKE '{species.species}' THEN 1 ELSE 0 END AS int) as {species.species}\"\n",
    "    \n",
    "query += \" FROM table_iris\"\n",
    "\n",
    "df_sql = spark.sql(query)\n",
    "print(\"Submitted query:\\n\\n\", query.replace(\", \", \",\\n\"), \"\\n\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645621c-e9f5-4e59-add0-aa12206a3662",
   "metadata": {},
   "source": [
    "**3. Using Pandas-on-Spark**<br/>\n",
    "Pandas on Spark is a new API that was introduce in Spark 3.2.0 (the current latest version of Spark). It allows users to process Spark DataFrames using\n",
    "Pandas functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8596d999-7e7b-4d8c-aa04-bbc3878d6330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "21/11/24 20:08:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/11/24 20:08:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/11/24 20:08:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "      <th>virginica</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>setosa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length sepal_width petal_length petal_width species  virginica  versicolor  setosa\n",
       "0           5.1         3.5          1.4         0.2  setosa          0           0       1\n",
       "1           4.9         3.0          1.4         0.2  setosa          0           0       1\n",
       "2           4.7         3.2          1.3         0.2  setosa          0           0       1\n",
       "3           4.6         3.1          1.5         0.2  setosa          0           0       1\n",
       "4           5.0         3.6          1.4         0.2  setosa          0           0       1\n",
       "5           5.4         3.9          1.7         0.4  setosa          0           0       1\n",
       "6           4.6         3.4          1.4         0.3  setosa          0           0       1\n",
       "7           5.0         3.4          1.5         0.2  setosa          0           0       1\n",
       "8           4.4         2.9          1.4         0.2  setosa          0           0       1\n",
       "9           4.9         3.1          1.5         0.1  setosa          0           0       1\n",
       "10          5.4         3.7          1.5         0.2  setosa          0           0       1\n",
       "11          4.8         3.4          1.6         0.2  setosa          0           0       1\n",
       "12          4.8         3.0          1.4         0.1  setosa          0           0       1\n",
       "13          4.3         3.0          1.1         0.1  setosa          0           0       1\n",
       "14          5.8         4.0          1.2         0.2  setosa          0           0       1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = df.to_pandas_on_spark()\n",
    "for species in all_species:\n",
    "    psdf[species.species] = 0\n",
    "    psdf[species.species] = psdf[species].where(psdf.species != species.species, 1)\n",
    "psdf[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41ce77-4938-4cb5-b13a-dde98f2f6988",
   "metadata": {},
   "source": [
    "#### Converting between the RDD and DataFrame APIs\n",
    "It is very simple to convert between both APIs. In this example we will access the RDD API using our existing DataFrame and use a `map` task to drop the species column, we will then revert back to the DataFrame API to display the data neatly with `show`.\n",
    "\n",
    "**DataFrame to RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ed44b36-93ac-4323-ad04-aeecfe9c136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length='5.1', sepal_width='3.5', petal_length='1.4', petal_width='0.2', virginica=0, versicolor=0, setosa=1),\n",
       " Row(sepal_length='4.9', sepal_width='3.0', petal_length='1.4', petal_width='0.2', virginica=0, versicolor=0, setosa=1),\n",
       " Row(sepal_length='4.7', sepal_width='3.2', petal_length='1.3', petal_width='0.2', virginica=0, versicolor=0, setosa=1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd_fromdf = df_api.rdd # the RDD API can now be accessed.\n",
    "\n",
    "# Create a new Row object without the Species column\n",
    "rdd_fromdf = rdd_fromdf.map(lambda x: Row(**{k:x[k] for k,v in x.asDict().items() if k != \"species\"}))\n",
    "rdd_fromdf.collect()[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef77a6b-a10e-44ae-afe9-088c8839187b",
   "metadata": {},
   "source": [
    "**RDD to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7303bca8-dc8b-4984-813d-e69f81114082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+---------+----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|virginica|versicolor|setosa|\n",
      "+------------+-----------+------------+-----------+---------+----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|        0|         0|     1|\n",
      "|         4.9|        3.0|         1.4|        0.2|        0|         0|     1|\n",
      "|         4.7|        3.2|         1.3|        0.2|        0|         0|     1|\n",
      "|         4.6|        3.1|         1.5|        0.2|        0|         0|     1|\n",
      "|         5.0|        3.6|         1.4|        0.2|        0|         0|     1|\n",
      "|         5.4|        3.9|         1.7|        0.4|        0|         0|     1|\n",
      "|         4.6|        3.4|         1.4|        0.3|        0|         0|     1|\n",
      "|         5.0|        3.4|         1.5|        0.2|        0|         0|     1|\n",
      "|         4.4|        2.9|         1.4|        0.2|        0|         0|     1|\n",
      "|         4.9|        3.1|         1.5|        0.1|        0|         0|     1|\n",
      "|         5.4|        3.7|         1.5|        0.2|        0|         0|     1|\n",
      "|         4.8|        3.4|         1.6|        0.2|        0|         0|     1|\n",
      "|         4.8|        3.0|         1.4|        0.1|        0|         0|     1|\n",
      "|         4.3|        3.0|         1.1|        0.1|        0|         0|     1|\n",
      "|         5.8|        4.0|         1.2|        0.2|        0|         0|     1|\n",
      "|         5.7|        4.4|         1.5|        0.4|        0|         0|     1|\n",
      "|         5.4|        3.9|         1.3|        0.4|        0|         0|     1|\n",
      "|         5.1|        3.5|         1.4|        0.3|        0|         0|     1|\n",
      "|         5.7|        3.8|         1.7|        0.3|        0|         0|     1|\n",
      "|         5.1|        3.8|         1.5|        0.3|        0|         0|     1|\n",
      "+------------+-----------+------------+-----------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fromrdd = rdd_fromdf.toDF()\n",
    "df_fromrdd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38923dca-19ad-4385-81ac-c99c41c2e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminating a SparkSession\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03fdae-1a80-4633-a425-afb61ed8d20f",
   "metadata": {},
   "source": [
    "### Machine Learning With Spark MLlib (DataFrame-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67ab6f-fc46-43f6-b7c8-0107536bf5c8",
   "metadata": {},
   "source": [
    "Spark's MLlib API provides users with the ability to apply a variety of machine learning algorithms on their data.\n",
    "Spark provides both an RDD and DataFrame-based API. We will be focusing on the DataFrame API.\n",
    "\n",
    "In this section, we will be reusing the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) dataset to train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f8f5bb-7f28-44fa-9a94-65888a9a1c92",
   "metadata": {},
   "source": [
    "#### Data Preparation with PySpark\n",
    "\n",
    "The first step we want to do is load the data and remove any columns with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95aa7ffd-60fc-4179-b646-f070fdad6abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+---------------+-----------------+-----------+------+\n",
      "|species|   island|culmen_length_mm|culmen_depth_mm|flipper_length_mm|body_mass_g|   sex|\n",
      "+-------+---------+----------------+---------------+-----------------+-----------+------+\n",
      "| Adelie|Torgersen|            39.1|           18.7|            181.0|     3750.0|  MALE|\n",
      "| Adelie|Torgersen|            39.5|           17.4|            186.0|     3800.0|FEMALE|\n",
      "| Adelie|Torgersen|            40.3|           18.0|            195.0|     3250.0|FEMALE|\n",
      "| Adelie|Torgersen|            36.7|           19.3|            193.0|     3450.0|FEMALE|\n",
      "| Adelie|Torgersen|            39.3|           20.6|            190.0|     3650.0|  MALE|\n",
      "| Adelie|Torgersen|            38.9|           17.8|            181.0|     3625.0|FEMALE|\n",
      "| Adelie|Torgersen|            39.2|           19.6|            195.0|     4675.0|  MALE|\n",
      "| Adelie|Torgersen|            41.1|           17.6|            182.0|     3200.0|FEMALE|\n",
      "| Adelie|Torgersen|            38.6|           21.2|            191.0|     3800.0|  MALE|\n",
      "| Adelie|Torgersen|            34.6|           21.1|            198.0|     4400.0|  MALE|\n",
      "| Adelie|Torgersen|            36.6|           17.8|            185.0|     3700.0|FEMALE|\n",
      "| Adelie|Torgersen|            38.7|           19.0|            195.0|     3450.0|FEMALE|\n",
      "| Adelie|Torgersen|            42.5|           20.7|            197.0|     4500.0|  MALE|\n",
      "| Adelie|Torgersen|            34.4|           18.4|            184.0|     3325.0|FEMALE|\n",
      "| Adelie|Torgersen|            46.0|           21.5|            194.0|     4200.0|  MALE|\n",
      "| Adelie|   Biscoe|            37.8|           18.3|            174.0|     3400.0|FEMALE|\n",
      "| Adelie|   Biscoe|            37.7|           18.7|            180.0|     3600.0|  MALE|\n",
      "| Adelie|   Biscoe|            35.9|           19.2|            189.0|     3800.0|FEMALE|\n",
      "| Adelie|   Biscoe|            38.2|           18.1|            185.0|     3950.0|  MALE|\n",
      "| Adelie|   Biscoe|            38.8|           17.2|            180.0|     3800.0|  MALE|\n",
      "+-------+---------+----------------+---------------+-----------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"My Spark DataFrame Program\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data/penguins.csv\", header=True)\n",
    "df = df.replace(\"?\", None).replace(\"_\", None).dropna()\n",
    "df = df.select(*(col(c).cast(\"float\").alias(c) \n",
    "                 if c != \"species\" and c != \"island\" and c != \"sex\" \n",
    "                 else c\n",
    "                 for c in df.columns ))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61838ce9-3d0a-41c5-9ea2-6f9cdb187507",
   "metadata": {},
   "source": [
    "Next, we will need to one-hot encode both the sex and island columns. To do so, we must first map the categorical data to categorical indices and then we can one-hot encode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff03fc2-f3da-4dfb-b8e7-5ed1b22db5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+---------------+-----------------+-----------+------+----------+-------+-----------+\n",
      "|species|   island|culmen_length_mm|culmen_depth_mm|flipper_length_mm|body_mass_g|   sex|island_idx|sex_idx|species_idx|\n",
      "+-------+---------+----------------+---------------+-----------------+-----------+------+----------+-------+-----------+\n",
      "| Adelie|Torgersen|            39.1|           18.7|            181.0|     3750.0|  MALE|       2.0|    1.0|        0.0|\n",
      "| Adelie|Torgersen|            39.5|           17.4|            186.0|     3800.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            40.3|           18.0|            195.0|     3250.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            36.7|           19.3|            193.0|     3450.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            39.3|           20.6|            190.0|     3650.0|  MALE|       2.0|    1.0|        0.0|\n",
      "| Adelie|Torgersen|            38.9|           17.8|            181.0|     3625.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            39.2|           19.6|            195.0|     4675.0|  MALE|       2.0|    1.0|        0.0|\n",
      "| Adelie|Torgersen|            41.1|           17.6|            182.0|     3200.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            38.6|           21.2|            191.0|     3800.0|  MALE|       2.0|    1.0|        0.0|\n",
      "| Adelie|Torgersen|            34.6|           21.1|            198.0|     4400.0|  MALE|       2.0|    1.0|        0.0|\n",
      "| Adelie|Torgersen|            36.6|           17.8|            185.0|     3700.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            38.7|           19.0|            195.0|     3450.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            42.5|           20.7|            197.0|     4500.0|  MALE|       2.0|    1.0|        0.0|\n",
      "| Adelie|Torgersen|            34.4|           18.4|            184.0|     3325.0|FEMALE|       2.0|    0.0|        0.0|\n",
      "| Adelie|Torgersen|            46.0|           21.5|            194.0|     4200.0|  MALE|       2.0|    1.0|        0.0|\n",
      "| Adelie|   Biscoe|            37.8|           18.3|            174.0|     3400.0|FEMALE|       0.0|    0.0|        0.0|\n",
      "| Adelie|   Biscoe|            37.7|           18.7|            180.0|     3600.0|  MALE|       0.0|    1.0|        0.0|\n",
      "| Adelie|   Biscoe|            35.9|           19.2|            189.0|     3800.0|FEMALE|       0.0|    0.0|        0.0|\n",
      "| Adelie|   Biscoe|            38.2|           18.1|            185.0|     3950.0|  MALE|       0.0|    1.0|        0.0|\n",
      "| Adelie|   Biscoe|            38.8|           17.2|            180.0|     3800.0|  MALE|       0.0|    1.0|        0.0|\n",
      "+-------+---------+----------------+---------------+-----------------+-----------+------+----------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCols=[\"island\", \"sex\", \"species\"], \n",
    "                              outputCols=[\"island_idx\", \"sex_idx\", \"species_idx\"],\n",
    "                              stringOrderType=\"alphabetAsc\")\n",
    "model = stringIndexer.fit(df)\n",
    "td = model.transform(df)\n",
    "\n",
    "td.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a4670d0-e8ce-4db4-81f7-550a54b8c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------------+-------------+\n",
      "|   island|   sex|   island_enc|      sex_enc|\n",
      "+---------+------+-------------+-------------+\n",
      "|Torgersen|  MALE|    (2,[],[])|    (1,[],[])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|  MALE|    (2,[],[])|    (1,[],[])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|  MALE|    (2,[],[])|    (1,[],[])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|  MALE|    (2,[],[])|    (1,[],[])|\n",
      "|Torgersen|  MALE|    (2,[],[])|    (1,[],[])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|  MALE|    (2,[],[])|    (1,[],[])|\n",
      "|Torgersen|FEMALE|    (2,[],[])|(1,[0],[1.0])|\n",
      "|Torgersen|  MALE|    (2,[],[])|    (1,[],[])|\n",
      "|   Biscoe|FEMALE|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|   Biscoe|  MALE|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   Biscoe|FEMALE|(2,[0],[1.0])|(1,[0],[1.0])|\n",
      "|   Biscoe|  MALE|(2,[0],[1.0])|    (1,[],[])|\n",
      "|   Biscoe|  MALE|(2,[0],[1.0])|    (1,[],[])|\n",
      "+---------+------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(inputCols=[\"island_idx\", \"sex_idx\"], outputCols=[\"island_enc\", \"sex_enc\"])\n",
    "model = ohe.fit(td)\n",
    "\n",
    "enc = model.transform(td)\n",
    "enc['island', 'sex', 'island_enc', 'sex_enc'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674a823-6d51-4624-b954-b1d9fa624e0b",
   "metadata": {},
   "source": [
    "We next need to merge all of our features into a a column of vectors to pass it to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eb3c983-6e72-4ca3-bea0-859701330c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[39.0999984741210...|\n",
      "|[39.5,17.39999961...|\n",
      "|[40.2999992370605...|\n",
      "|[36.7000007629394...|\n",
      "|[39.2999992370605...|\n",
      "|[38.9000015258789...|\n",
      "|[39.2000007629394...|\n",
      "|[41.0999984741210...|\n",
      "|[38.5999984741210...|\n",
      "|[34.5999984741210...|\n",
      "|[36.5999984741210...|\n",
      "|[38.7000007629394...|\n",
      "|[42.5,20.70000076...|\n",
      "|[34.4000015258789...|\n",
      "|[46.0,21.5,194.0,...|\n",
      "|[37.7999992370605...|\n",
      "|[37.7000007629394...|\n",
      "|[35.9000015258789...|\n",
      "|[38.2000007629394...|\n",
      "|[38.7999992370605...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_columns = [name for name in enc.columns if \"species\" not in name and \n",
    "                 \"idx\" not in name and\n",
    "                 \"sex\" != name and\n",
    "                 \"island\" != name]\n",
    "assembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
    "va = assembler.transform(enc)\n",
    "\n",
    "va.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e4e73-0cef-4de6-ab16-d8442e177117",
   "metadata": {},
   "source": [
    "Finally, we can split our data into training and test sets and then we are ready to train and evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a76b1963-d1c4-44c9-a55a-d205e027eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------+---------------+-----------------+-----------+------+----------+-------+-----------+-------------+-------------+--------------------+\n",
      "|species|   island|culmen_length_mm|culmen_depth_mm|flipper_length_mm|body_mass_g|   sex|island_idx|sex_idx|species_idx|   island_enc|      sex_enc|            features|\n",
      "+-------+---------+----------------+---------------+-----------------+-----------+------+----------+-------+-----------+-------------+-------------+--------------------+\n",
      "| Adelie|   Biscoe|            38.2|           20.0|            190.0|     3900.0|  MALE|       0.0|    1.0|        0.0|(2,[0],[1.0])|    (1,[],[])|[38.2000007629394...|\n",
      "| Adelie|   Biscoe|            38.6|           17.2|            199.0|     3750.0|FEMALE|       0.0|    0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|[38.5999984741210...|\n",
      "| Adelie|   Biscoe|            40.1|           18.9|            188.0|     4300.0|  MALE|       0.0|    1.0|        0.0|(2,[0],[1.0])|    (1,[],[])|[40.0999984741210...|\n",
      "| Adelie|   Biscoe|            40.5|           17.9|            187.0|     3200.0|FEMALE|       0.0|    0.0|        0.0|(2,[0],[1.0])|(1,[0],[1.0])|[40.5,17.89999961...|\n",
      "| Adelie|   Biscoe|            40.5|           18.9|            180.0|     3950.0|  MALE|       0.0|    1.0|        0.0|(2,[0],[1.0])|    (1,[],[])|[40.5,18.89999961...|\n",
      "| Adelie|   Biscoe|            41.3|           21.1|            195.0|     4400.0|  MALE|       0.0|    1.0|        0.0|(2,[0],[1.0])|    (1,[],[])|[41.2999992370605...|\n",
      "| Adelie|   Biscoe|            41.4|           18.6|            191.0|     3700.0|  MALE|       0.0|    1.0|        0.0|(2,[0],[1.0])|    (1,[],[])|[41.4000015258789...|\n",
      "| Adelie|   Biscoe|            42.0|           19.5|            200.0|     4050.0|  MALE|       0.0|    1.0|        0.0|(2,[0],[1.0])|    (1,[],[])|[42.0,19.5,200.0,...|\n",
      "| Adelie|    Dream|            36.0|           17.1|            187.0|     3700.0|FEMALE|       1.0|    0.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|[36.0,17.10000038...|\n",
      "| Adelie|    Dream|            36.2|           17.3|            187.0|     3300.0|FEMALE|       1.0|    0.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|[36.2000007629394...|\n",
      "| Adelie|    Dream|            37.0|           16.5|            185.0|     3400.0|FEMALE|       1.0|    0.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|[37.0,16.5,185.0,...|\n",
      "| Adelie|    Dream|            37.0|           16.9|            185.0|     3000.0|FEMALE|       1.0|    0.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|[37.0,16.89999961...|\n",
      "| Adelie|    Dream|            37.2|           18.1|            178.0|     3900.0|  MALE|       1.0|    1.0|        0.0|(2,[1],[1.0])|    (1,[],[])|[37.2000007629394...|\n",
      "| Adelie|    Dream|            37.5|           18.5|            199.0|     4475.0|  MALE|       1.0|    1.0|        0.0|(2,[1],[1.0])|    (1,[],[])|[37.5,18.5,199.0,...|\n",
      "| Adelie|    Dream|            38.9|           18.8|            190.0|     3600.0|FEMALE|       1.0|    0.0|        0.0|(2,[1],[1.0])|(1,[0],[1.0])|[38.9000015258789...|\n",
      "| Adelie|    Dream|            39.2|           21.1|            196.0|     4150.0|  MALE|       1.0|    1.0|        0.0|(2,[1],[1.0])|    (1,[],[])|[39.2000007629394...|\n",
      "| Adelie|    Dream|            40.8|           18.4|            195.0|     3900.0|  MALE|       1.0|    1.0|        0.0|(2,[1],[1.0])|    (1,[],[])|[40.7999992370605...|\n",
      "| Adelie|    Dream|            40.9|           18.9|            184.0|     3900.0|  MALE|       1.0|    1.0|        0.0|(2,[1],[1.0])|    (1,[],[])|[40.9000015258789...|\n",
      "| Adelie|    Dream|            41.1|           17.5|            190.0|     3900.0|  MALE|       1.0|    1.0|        0.0|(2,[1],[1.0])|    (1,[],[])|[41.0999984741210...|\n",
      "| Adelie|Torgersen|            34.4|           18.4|            184.0|     3325.0|FEMALE|       2.0|    0.0|        0.0|    (2,[],[])|(1,[0],[1.0])|[34.4000015258789...|\n",
      "+-------+---------+----------------+---------------+-----------------+-----------+------+----------+-------+-----------+-------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = va.randomSplit([0.8, 0.2], seed=0)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b984a34-9588-4e76-ba5f-ef46ec8d8544",
   "metadata": {},
   "source": [
    "#### Random Forest Classification with PySpark\n",
    "\n",
    "Now that we have encoded our data, we can proceed to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eba32e4-3d6a-4ecb-8aa0-f99a9d40ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol =\"features\", labelCol =\"species_idx\")\n",
    "rf_model = rf.fit(train)\n",
    "predictions = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5b907-e62b-4b42-91f1-4dc2d3bc8f0a",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can evaluate it using Spark's `MulticlassClassificationEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7de1abe-a90a-4e93-aa17-b1090b963f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n",
      "Test Error = 0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"species_idx\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b5f09-1ae6-4c1f-8b0f-6e6e71df8262",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- [RDD Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [MLlib Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [PySpark API](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
